{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:05:46.641896Z","iopub.execute_input":"2025-03-12T07:05:46.642231Z","iopub.status.idle":"2025-03-12T07:05:46.646983Z","shell.execute_reply.started":"2025-03-12T07:05:46.642204Z","shell.execute_reply":"2025-03-12T07:05:46.646152Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets torch evaluate accelerate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM, \n    Trainer, TrainingArguments, DataCollatorForSeq2Seq\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T14:59:17.473937Z","iopub.execute_input":"2025-03-12T14:59:17.474219Z","iopub.status.idle":"2025-03-12T14:59:17.478124Z","shell.execute_reply.started":"2025-03-12T14:59:17.474197Z","shell.execute_reply":"2025-03-12T14:59:17.477234Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Initial model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load the BERT-based model\nmodel_name = \"t5-small\"  # You can also try \"facebook/bart-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T12:16:57.053350Z","iopub.execute_input":"2025-03-12T12:16:57.054049Z","iopub.status.idle":"2025-03-12T12:16:58.047472Z","shell.execute_reply.started":"2025-03-12T12:16:57.054016Z","shell.execute_reply":"2025-03-12T12:16:58.046741Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"esnli\")  # Explainable NLI dataset\ntrain_data = dataset[\"train\"]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_data(examples):\n    inputs = [\"Premise: \" + p + \" Hypothesis: \" + h for p, h in zip(examples[\"premise\"], examples[\"hypothesis\"])]\n    outputs = examples[\"explanation_1\"]\n    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n    labels = tokenizer(outputs, truncation=True, padding=\"max_length\", max_length=128)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_data, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(5000))  # Use only 5000 samples\nsmall_eval_dataset = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(1000))  # Use only 1000 samples","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T12:17:05.001245Z","iopub.execute_input":"2025-03-12T12:17:05.001651Z","iopub.status.idle":"2025-03-12T12:17:05.005796Z","shell.execute_reply.started":"2025-03-12T12:17:05.001620Z","shell.execute_reply":"2025-03-12T12:17:05.004762Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=4,  # Keep small for Kaggle GPU\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,  # Adjust for better training\n    weight_decay=0.01,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_explanation(premise, hypothesis):\n    input_text = f\"Premise: {premise} Hypothesis: {hypothesis}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Move to GPU\n    output = model.generate(**inputs, max_length=128)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Example\npremise = \"The weather is clear and sunny.\"\nhypothesis = \"It is not raining.\"\nprint(generate_explanation(premise, hypothesis))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_bert_xai\")\ntokenizer.save_pretrained(\"./fine_tuned_bert_xai\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_explanation(premise, hypothesis):\n    input_text = f\"Explain why: {premise} Therefore, {hypothesis}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    output = model.generate(**inputs, max_length=128)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Example Test\npremise = \"The sky is cloudy.\"\nhypothesis = \"It might rain soon.\"\nprint(generate_explanation(premise, hypothesis))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Improving results with a different dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"cos_e\", \"v1.11\")\nprint(dataset[\"train\"].column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T12:17:11.180257Z","iopub.execute_input":"2025-03-12T12:17:11.180635Z","iopub.status.idle":"2025-03-12T12:17:21.022349Z","shell.execute_reply.started":"2025-03-12T12:17:11.180605Z","shell.execute_reply":"2025-03-12T12:17:21.021607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e83f707bc74a218cf6ad5a3221eca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ebfc4d2642e491d997298cf28ca653e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/222k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354b599e692740648f25fde8ca14ad0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a14e996e88e047e8974ad8568ef5c093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e865c6060bec4870958b302c70ff2365"}},"metadata":{}},{"name":"stdout","text":"['id', 'question', 'choices', 'answer', 'abstractive_explanation', 'extractive_explanation']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def preprocess_data(examples):\n    inputs = [f\"Explain: {q} Choices: {c} Answer: {examples['answer'][i]}\" \n              for i, (q, c) in enumerate(zip(examples[\"question\"], examples[\"choices\"]))]\n    outputs = examples[\"abstractive_explanation\"]  # Use correct column name\n    \n    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n    labels = tokenizer(outputs, truncation=True, padding=\"max_length\", max_length=128)\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T12:17:23.369319Z","iopub.execute_input":"2025-03-12T12:17:23.369688Z","iopub.status.idle":"2025-03-12T12:17:27.177900Z","shell.execute_reply.started":"2025-03-12T12:17:23.369660Z","shell.execute_reply":"2025-03-12T12:17:27.176727Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9741 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd38690e4464f85bfc5bcbdb14ab8f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1221 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd68f8b995fe4ebf9dd8c29451a74783"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_cosE\",\n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,  \n    num_train_epochs=3,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# More epochs, with weight decay","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_cosE2\",\n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,  \n    num_train_epochs=5,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_cosE3\",\n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,  \n    num_train_epochs=5,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_explanation(question, choices, answer):\n    input_text = f\"Question: {question} Answer: {answer}. Explain why in a clear and detailed way.\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\n    output = model.generate(\n        **inputs,\n        max_length=128,  \n        min_length=50,   # Forces detailed explanations\n        do_sample=True,  \n        temperature=0.7,  \n        top_p=0.9,  \n        repetition_penalty=1.5,  # Penalizes repeating phrases\n    )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nquestion = \"Why do we wear sunglasses?\"\nchoices = \"['To look stylish', 'To protect our eyes', 'To hide from people']\"\nanswer = \"To protect our eyes\"\n\nprint(generate_explanation(question, choices, answer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_cosE_best\",\n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,  \n    num_train_epochs=5,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.001\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:07:38.008245Z","iopub.execute_input":"2025-03-12T07:07:38.008599Z","iopub.status.idle":"2025-03-12T07:36:36.010662Z","shell.execute_reply.started":"2025-03-12T07:07:38.008570Z","shell.execute_reply":"2025-03-12T07:36:36.009862Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6090' max='6090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6090/6090 28:54, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.647000</td>\n      <td>0.253465</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.242000</td>\n      <td>0.242248</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.229100</td>\n      <td>0.240974</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.224800</td>\n      <td>0.240255</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.222600</td>\n      <td>0.239898</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6090, training_loss=0.31311624461206894, metrics={'train_runtime': 1737.0511, 'train_samples_per_second': 28.039, 'train_steps_per_second': 3.506, 'total_flos': 6591822440693760.0, 'train_loss': 0.31311624461206894, 'epoch': 5.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"model_path = \"./cose_model\"  # Make sure this is the correct path\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:36:54.840862Z","iopub.execute_input":"2025-03-12T07:36:54.841201Z","iopub.status.idle":"2025-03-12T07:36:55.260594Z","shell.execute_reply.started":"2025-03-12T07:36:54.841171Z","shell.execute_reply":"2025-03-12T07:36:55.259621Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def generate_explanation(question, choices, answer):\n    input_text = f\"Question: {question} Answer: {answer}.\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n    output = model.generate(\n        **inputs,\n        max_length=128,  \n        min_length=10,   # Forces detailed explanations\n        do_sample=True,  \n        temperature=0.7,  \n        top_p=0.9,  \n        repetition_penalty=1.5,  # Penalizes repeating phrases\n    )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nquestion = \"Why did the AI flag this transaction as fraud? \"\nchoices = \"['Transaction amount was too high', 'Location was unusual', 'User behavior was suspicious']\"\nanswer = \"User behavior was suspicious\"\n\nprint(generate_explanation(question, choices, answer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:37:09.459699Z","iopub.execute_input":"2025-03-12T07:37:09.460046Z","iopub.status.idle":"2025-03-12T07:37:09.614659Z","shell.execute_reply.started":"2025-03-12T07:37:09.460019Z","shell.execute_reply":"2025-03-12T07:37:09.613933Z"}},"outputs":[{"name":"stdout","text":"user behavior was suspicious. the AI flag this transaction as fraud\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model.save_pretrained(\"./cose_model\")\ntokenizer.save_pretrained(\"./cose_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:36:50.404216Z","iopub.execute_input":"2025-03-12T07:36:50.404599Z","iopub.status.idle":"2025-03-12T07:36:51.250048Z","shell.execute_reply.started":"2025-03-12T07:36:50.404567Z","shell.execute_reply":"2025-03-12T07:36:51.249400Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./cose_model/tokenizer_config.json',\n './cose_model/special_tokens_map.json',\n './cose_model/spiece.model',\n './cose_model/added_tokens.json',\n './cose_model/tokenizer.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_cosE_longer\",\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,  \n    num_train_epochs=15,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.001\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T12:17:43.023523Z","iopub.execute_input":"2025-03-12T12:17:43.023845Z","iopub.status.idle":"2025-03-12T13:32:28.430652Z","shell.execute_reply.started":"2025-03-12T12:17:43.023823Z","shell.execute_reply":"2025-03-12T13:32:28.429714Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9135' max='9135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9135/9135 1:14:42, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.911500</td>\n      <td>0.275559</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.255700</td>\n      <td>0.247064</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.234200</td>\n      <td>0.242684</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.226700</td>\n      <td>0.240452</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.222000</td>\n      <td>0.239260</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.219300</td>\n      <td>0.240031</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.216600</td>\n      <td>0.239584</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.214800</td>\n      <td>0.237638</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.213500</td>\n      <td>0.238217</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.211700</td>\n      <td>0.237921</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.211000</td>\n      <td>0.238227</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.209800</td>\n      <td>0.238452</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.209600</td>\n      <td>0.238184</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.209000</td>\n      <td>0.238214</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.208500</td>\n      <td>0.238262</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9135, training_loss=0.2649263278604141, metrics={'train_runtime': 4484.4588, 'train_samples_per_second': 32.583, 'train_steps_per_second': 2.037, 'total_flos': 1.977546732208128e+16, 'train_loss': 0.2649263278604141, 'epoch': 15.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model.save_pretrained(\"./cose_model_v2\")\ntokenizer.save_pretrained(\"./cose_model_v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T13:32:28.432132Z","iopub.execute_input":"2025-03-12T13:32:28.432407Z","iopub.status.idle":"2025-03-12T13:32:29.039366Z","shell.execute_reply.started":"2025-03-12T13:32:28.432369Z","shell.execute_reply":"2025-03-12T13:32:29.038587Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('./cose_model_v2/tokenizer_config.json',\n './cose_model_v2/special_tokens_map.json',\n './cose_model_v2/spiece.model',\n './cose_model_v2/added_tokens.json',\n './cose_model_v2/tokenizer.json')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model_path = \"./cose_model_v2\"  # Make sure this is the correct path\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T14:59:27.209768Z","iopub.execute_input":"2025-03-12T14:59:27.210059Z","iopub.status.idle":"2025-03-12T14:59:27.673087Z","shell.execute_reply.started":"2025-03-12T14:59:27.210036Z","shell.execute_reply":"2025-03-12T14:59:27.672430Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def generate_explanation(question, choices, answer):\n    input_text = f\"Question: {question} Answer: {answer}.\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n    output = model.generate(\n        **inputs,\n        max_length=128,  \n        min_length=10,   # Forces detailed explanations\n        do_sample=True,  \n        temperature=0.7,  \n        top_p=0.9,  \n        repetition_penalty=1.5,  # Penalizes repeating phrases\n    )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nquestion = \"Why did the AI flag this transaction as fraud? \"\nchoices = \"['Transaction amount was too high', 'Location was unusual', 'User behavior was suspicious']\"\nanswer = \"User behavior was suspicious\"\n\nprint(generate_explanation(question, choices, answer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T14:59:42.787120Z","iopub.execute_input":"2025-03-12T14:59:42.787464Z","iopub.status.idle":"2025-03-12T14:59:42.935022Z","shell.execute_reply.started":"2025-03-12T14:59:42.787436Z","shell.execute_reply":"2025-03-12T14:59:42.934163Z"}},"outputs":[{"name":"stdout","text":"the AI flag this transaction as fraud. User behavior was suspicious\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Cos E with facebook bart","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T08:42:43.634016Z","iopub.execute_input":"2025-03-12T08:42:43.634372Z","iopub.status.idle":"2025-03-12T08:42:44.815186Z","shell.execute_reply.started":"2025-03-12T08:42:43.634342Z","shell.execute_reply":"2025-03-12T08:42:44.814430Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_bart\",\n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,  \n    num_train_epochs=5,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:37:52.976012Z","iopub.execute_input":"2025-03-12T07:37:52.976319Z","iopub.status.idle":"2025-03-12T08:33:09.095183Z","shell.execute_reply.started":"2025-03-12T07:37:52.976296Z","shell.execute_reply":"2025-03-12T08:33:09.094216Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6090' max='6090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6090/6090 55:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.409800</td>\n      <td>0.365140</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.331500</td>\n      <td>0.367349</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.311800</td>\n      <td>0.368198</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.298700</td>\n      <td>0.370502</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.290600</td>\n      <td>0.371291</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6090, training_loss=0.3284676838390933, metrics={'train_runtime': 3315.6211, 'train_samples_per_second': 14.69, 'train_steps_per_second': 1.837, 'total_flos': 1.48486070992896e+16, 'train_loss': 0.3284676838390933, 'epoch': 5.0})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"model.save_pretrained(\"./bart_model_v1\")\ntokenizer.save_pretrained(\"./bart_model_v1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T08:33:09.096337Z","iopub.execute_input":"2025-03-12T08:33:09.096676Z","iopub.status.idle":"2025-03-12T08:33:10.329137Z","shell.execute_reply.started":"2025-03-12T08:33:09.096646Z","shell.execute_reply":"2025-03-12T08:33:10.328415Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"('./bart_model_v1/tokenizer_config.json',\n './bart_model_v1/special_tokens_map.json',\n './bart_model_v1/vocab.json',\n './bart_model_v1/merges.txt',\n './bart_model_v1/added_tokens.json',\n './bart_model_v1/tokenizer.json')"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"model_path = \"./bart_model_v1\"  # Make sure this is the correct path\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T09:31:56.298202Z","iopub.execute_input":"2025-03-12T09:31:56.298537Z","iopub.status.idle":"2025-03-12T09:31:57.388771Z","shell.execute_reply.started":"2025-03-12T09:31:56.298511Z","shell.execute_reply":"2025-03-12T09:31:57.388034Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def generate_explanation(question, choices, answer):\n    input_text = f\"Question: {question} Answer: {answer}.\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n    output = model.generate(\n        **inputs,\n        max_length=128,  \n        min_length=10,   # Forces detailed explanations\n        do_sample=True,  \n        temperature=0.7,  \n        top_p=0.9,  \n        repetition_penalty=1.5,  # Penalizes repeating phrases\n    )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nquestion = \"Why did the AI flag this transaction as fraud? \"\nchoices = \"['Transaction amount was too high', 'Location was unusual', 'User behavior was suspicious']\"\nanswer = \"User behavior was suspicious\"\n\nprint(generate_explanation(question, choices, answer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T09:32:04.813574Z","iopub.execute_input":"2025-03-12T09:32:04.813880Z","iopub.status.idle":"2025-03-12T09:32:06.061427Z","shell.execute_reply.started":"2025-03-12T09:32:04.813857Z","shell.execute_reply":"2025-03-12T09:32:06.060649Z"}},"outputs":[{"name":"stdout","text":" Question: Why did the AI flag this transaction as fraud?  Answer: User behavior was suspicious.\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results_bart_v2\",\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,  \n    num_train_epochs=5,  \n    fp16=True,  # Enable mixed precision for faster training\n    save_total_limit=1,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.001\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T08:43:09.198975Z","iopub.execute_input":"2025-03-12T08:43:09.199324Z","iopub.status.idle":"2025-03-12T09:29:03.747508Z","shell.execute_reply.started":"2025-03-12T08:43:09.199301Z","shell.execute_reply":"2025-03-12T09:29:03.746792Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3045' max='3045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3045/3045 45:52, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.495700</td>\n      <td>0.364151</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.342500</td>\n      <td>0.366109</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.323800</td>\n      <td>0.368624</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.313500</td>\n      <td>0.370381</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.307200</td>\n      <td>0.369693</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3045, training_loss=0.35653723104442475, metrics={'train_runtime': 2753.4343, 'train_samples_per_second': 17.689, 'train_steps_per_second': 1.106, 'total_flos': 1.48486070992896e+16, 'train_loss': 0.35653723104442475, 'epoch': 5.0})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"model.save_pretrained(\"./bart_model_v2\")\ntokenizer.save_pretrained(\"./bart_model_v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T09:29:03.748722Z","iopub.execute_input":"2025-03-12T09:29:03.749038Z","iopub.status.idle":"2025-03-12T09:29:05.054865Z","shell.execute_reply.started":"2025-03-12T09:29:03.749007Z","shell.execute_reply":"2025-03-12T09:29:05.054068Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"('./bart_model_v2/tokenizer_config.json',\n './bart_model_v2/special_tokens_map.json',\n './bart_model_v2/vocab.json',\n './bart_model_v2/merges.txt',\n './bart_model_v2/added_tokens.json',\n './bart_model_v2/tokenizer.json')"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"model_path = \"./bart_model_v2\"  # Make sure this is the correct path\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T09:31:41.752353Z","iopub.execute_input":"2025-03-12T09:31:41.752706Z","iopub.status.idle":"2025-03-12T09:31:42.864114Z","shell.execute_reply.started":"2025-03-12T09:31:41.752663Z","shell.execute_reply":"2025-03-12T09:31:42.863277Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def generate_explanation(question, choices, answer):\n    input_text = f\"Question: {question} Answer: {answer}.\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}\n    output = model.generate(\n        **inputs,\n        max_length=128,  \n        min_length=10,   # Forces detailed explanations\n        do_sample=True,  \n        temperature=0.7,  \n        top_p=0.9,  \n        repetition_penalty=1.5,  # Penalizes repeating phrases\n    )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nquestion = \"Why did the AI flag this transaction as fraud? \"\nchoices = \"['Transaction amount was too high', 'Location was unusual', 'User behavior was suspicious']\"\nanswer = \"User behavior was suspicious\"\n\nprint(generate_explanation(question, choices, answer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T09:31:49.391212Z","iopub.execute_input":"2025-03-12T09:31:49.391541Z","iopub.status.idle":"2025-03-12T09:31:50.606963Z","shell.execute_reply.started":"2025-03-12T09:31:49.391513Z","shell.execute_reply":"2025-03-12T09:31:50.605978Z"}},"outputs":[{"name":"stdout","text":"Question: Why did the AI flag this transaction as fraud?  Answer: User behavior was suspicious.\n","output_type":"stream"}],"execution_count":55}]}
